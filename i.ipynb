{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_VERSION\"] = \"3.1\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import count, round, concat\n",
    "from pyspark.sql.functions import mean, min, max, sum, datediff, to_date\n",
    "from pyspark.sql.functions import to_utc_timestamp, unix_timestamp, lit, datediff, col\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.functions import when\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydeequ\n",
    "\n",
    "import pydeequ\n",
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "from pydeequ.analyzers import *\n",
    "from pydeequ.anomaly_detection import *\n",
    "from pydeequ.profiles import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName(\"testData\") \\\n",
    "#     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "#     .getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"testData\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('testData.csv', header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_count(df_in):\n",
    "    df_in.agg( *[ count(c).alias(c) for c in df_in.columns ] ).show()\n",
    "    \n",
    "my_count(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()\n",
    "my_count(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in data.columns:\n",
    "    if data.schema[c].dataType in [int, float]:\n",
    "        data = data.withColumn(c, func.round(data[c], 3).cast('integer'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data.columns[0:9]\n",
    "data.select(cols).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupBy(\"price\").count().sort(col('price')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(df, col, bins=10, xname=None, yname=None):\n",
    "\n",
    "    vals = df.select(col).rdd.flatMap(lambda x: x).histogram(bins)\n",
    "\n",
    "    width = vals[0][1] - vals[0][0]\n",
    "    loc = [vals[0][0] + (i+1) * width for i in range(len(vals[1]))]\n",
    "    \n",
    "    plt.bar(loc, vals[1], width=width)\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(yname)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# histogram(data, 'price', bins=500,xname='price', yname='state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# # Perform aggregation\n",
    "# aggregated_data = data.agg(F.avg('acre_lot').alias('avg_acre_lot'), F.avg('house_size').alias('avg_house_size')) \\\n",
    "#     .sort('avg_acre_lot')\n",
    "\n",
    "# # Show the aggregated data\n",
    "# aggregated_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineplot(df, x, y, measure='mean'):    \n",
    "    if measure == 'mean':\n",
    "        pd_df = df.groupBy(x).mean(y).toPandas().sort_values(by=x)\n",
    "        pd_df.plot(x, 'avg({})'.format(y), legend=False)\n",
    "    elif measure == 'total':\n",
    "        pd_df = df.groupBy(x).sum(y).toPandas().sort_values(by=x)\n",
    "        pd_df.plot(x, 'sum({})'.format(y), legend=False)\n",
    "    elif measure == 'count':\n",
    "        pd_df = df.groupBy(x).count().toPandas().sort_values(by=x)\n",
    "        pd_df.plot(x, 'count', legend=False)\n",
    "    plt.ylabel(y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lineplot(data, 'price', 'bed', measure='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lineplot(data, 'acre_lot', 'price', measure='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_lot_count =(data.groupBy('status').count().sort('status').cache())\n",
    "                                         \n",
    "area_lot_count = area_lot_count.count()\n",
    "print('Found %d response codes' % area_lot_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram(data, 'status', bins=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove all whitespaces from every single column in the Dataframe\n",
    "tempList = [] \n",
    "for col in df.columns:\n",
    "  new_name = col.strip()\n",
    "  new_name = \"\".join(new_name.split())\n",
    "  new_name = new_name.replace('.','')  \n",
    "  tempList.append(new_name) \n",
    "print(tempList)\n",
    "\n",
    "data = data.toDF(*tempList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('status', round(data['acre_lot3'] / data['bed'], 2))\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.filter((data['status']==0.01) & (data['price']>1100) & (data['acre_lot7']>1190)).show()\n",
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('status', \n",
    "                       when((data['price'] > 10000) & (data['acre_lot7'] ==4000), 1) \\\n",
    "                       .when((data['acre_lot7'] > 0.1), 1) \\\n",
    "                       .otherwise(0))\n",
    "\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pieplot(df, col, lim=10, yname=None):\n",
    "\n",
    "    classes = df.groupBy(col).count().orderBy('count', ascending=False)\n",
    "    \n",
    "    pd_df = classes.limit(lim).toPandas()\n",
    "    \n",
    "    pd_df.plot(kind='pie', x=col, y='count', \\\n",
    "           labels=pd_df[col], legend=False)\n",
    "    plt.ylabel(None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_graph = data.filter((data['acre_lot7']==0.1) & (data['price'] > 1400))\n",
    "pieplot(data_graph, 'acre_lot7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot(df, col, group_by): \n",
    "    pd_df = df.toPandas()\n",
    "    pd_df.boxplot(col, by=group_by, figsize=(8, 5))\n",
    "    plt.ylabel(col)\n",
    "    plt.title(None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot(data, 'price','city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialization of a test\n",
    "# check = Check(spark, CheckLevel.Error, \"Integrity checks\")\n",
    "\n",
    "# # testData overview / data testing\n",
    "# checkResult = VerificationSuite(spark) \\\n",
    "#     .onData(data) \\\n",
    "#     .addCheck(\n",
    "#         check.hasSize(lambda x: x >= 50) \\\n",
    "#         .hasMin(\"price\", lambda x: x > 0) \\\n",
    "#         .isComplete(\"status\")  \\\n",
    "#         .isUnique(\"prev_sold_date\")  \\\n",
    "#         .isNonNegative(\"price\")) \\\n",
    "#     .run()\n",
    "\n",
    "# # Running verification\n",
    "# checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "# checkResult_df.show()\n",
    "\n",
    "# # Describing the result\n",
    "# if checkResult.status == \"Success\":\n",
    "#     print('Tests passed')\n",
    "# else:\n",
    "#     print('Errors found:')\n",
    "\n",
    "#     for check_json in checkResult.checkResults:\n",
    "#         if check_json['constraint_status'] != \"Success\":\n",
    "#             print(f\"\\t{check_json['constraint']} reason: {check_json['constraint_message']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysisResult = AnalysisRunner(spark) \\\n",
    "#                     .onData(data) \\\n",
    "#                     .addAnalyzer(Size()) \\\n",
    "#                     .addAnalyzer(Completeness(\"price\")) \\\n",
    "#                     .addAnalyzer(ApproxCountDistinct(\"city\")) \\\n",
    "#                     .addAnalyzer(Mean(\"acre_lot\")) \\\n",
    "#                     .addAnalyzer(Compliance(\"acre_lot\", \"acre_lot >= 140.0\")) \\\n",
    "#                     .addAnalyzer(Correlation(\"bed\", \"bath\")) \\\n",
    "#                     .run()\n",
    "                    \n",
    "# analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)\n",
    "# analysisResult_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = ColumnProfilerRunner(spark) \\\n",
    "#     .onData(data) \\\n",
    "#     .run()\n",
    "\n",
    "# for col, profile in result.profiles.items():\n",
    "#     print(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anomalyResult = VerificationSuite(spark) \\\n",
    "#     .onData(data) \\\n",
    "#     .addAnomalyCheck(BatchNormalStrategy(lowerDeviationFactor=3.0, upperDeviationFactor=3.0, includeInterval=False)) \\\n",
    "#     .run()\n",
    "\n",
    "# anomalyResult = VerificationResult.checkResultsAsDataFrame(spark, anomalyResult)\n",
    "# anomalyResult.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
